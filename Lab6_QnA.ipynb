{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":799923,"sourceType":"datasetVersion","datasetId":374}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T14:42:21.213427Z","iopub.execute_input":"2025-09-07T14:42:21.213744Z","iopub.status.idle":"2025-09-07T14:42:24.702020Z","shell.execute_reply.started":"2025-09-07T14:42:21.213725Z","shell.execute_reply":"2025-09-07T14:42:24.700983Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import argparse\nimport json\nimport numpy as np\nimport torch\nimport gc\nimport os\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForQuestionAnswering,\n    TrainingArguments,\n    Trainer,\n    default_data_collator,\n    pipeline,\n)\nimport evaluate\nfrom transformers import logging\nlogging.set_verbosity_error()\n\n# Force cleanup and set environment variables\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ndef cleanup_memory():\n    \"\"\"Aggressive memory cleanup\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\ncleanup_memory()\nprint(\"Libraries imported and memory cleaned\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-07T14:42:27.164759Z","iopub.execute_input":"2025-09-07T14:42:27.165036Z","iopub.status.idle":"2025-09-07T14:42:36.858193Z","shell.execute_reply.started":"2025-09-07T14:42:27.165007Z","shell.execute_reply":"2025-09-07T14:42:36.857268Z"}},"outputs":[{"name":"stderr","text":"2025-09-07 14:42:33.422423: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757256153.444794     352 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757256153.451435     352 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Libraries imported and memory cleaned\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def prepare_train_features(examples, tokenizer, max_length=256, doc_stride=64):\n    \"\"\"\n    Simplified tokenization with smaller sequences to reduce memory usage\n    \"\"\"\n    # Tokenize with smaller sequences\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n        return_tensors=None,  # Don't return tensors to save memory\n    )\n\n    # Map features back to examples\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    start_positions = []\n    end_positions = []\n\n    for i, offsets in enumerate(offset_mapping):\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        \n        if len(answers[\"answer_start\"]) == 0:\n            start_positions.append(cls_index)\n            end_positions.append(cls_index)\n        else:\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Find context tokens\n            token_start_index = 0\n            while token_start_index < len(sequence_ids) and sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            token_end_index = len(input_ids) - 1\n            while token_end_index >= 0 and sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            # Check bounds\n            if (token_start_index >= len(offsets) or token_end_index >= len(offsets) or\n                token_start_index > token_end_index or\n                not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char)):\n                start_positions.append(cls_index)\n                end_positions.append(cls_index)\n            else:\n                # Find token positions\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                start_positions.append(max(0, token_start_index - 1))\n\n                while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                end_positions.append(min(len(input_ids) - 1, token_end_index + 1))\n\n    tokenized_examples[\"start_positions\"] = start_positions\n    tokenized_examples[\"end_positions\"] = end_positions\n    return tokenized_examples\n\ndef flatten_squad_data(dataset, max_samples=None):\n    \"\"\"\n    Flatten SQuAD data with optional sample limiting\n    \"\"\"\n    flattened_data = {\n        \"id\": [],\n        \"title\": [],\n        \"context\": [],\n        \"question\": [],\n        \"answers\": []\n    }\n    \n    sample_count = 0\n    for article in dataset:\n        if max_samples and sample_count >= max_samples:\n            break\n            \n        title = article[\"title\"]\n        for paragraph in article[\"paragraphs\"]:\n            if max_samples and sample_count >= max_samples:\n                break\n                \n            context = paragraph[\"context\"]\n            for qa in paragraph[\"qas\"]:\n                if max_samples and sample_count >= max_samples:\n                    break\n                    \n                flattened_data[\"id\"].append(qa[\"id\"])\n                flattened_data[\"title\"].append(title)\n                flattened_data[\"context\"].append(context)\n                flattened_data[\"question\"].append(qa[\"question\"])\n                flattened_data[\"answers\"].append({\n                    \"text\": [answer[\"text\"] for answer in qa[\"answers\"]],\n                    \"answer_start\": [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n                })\n                sample_count += 1\n    \n    return flattened_data\n\nprint(\"Data preparation functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T14:42:36.859710Z","iopub.execute_input":"2025-09-07T14:42:36.860453Z","iopub.status.idle":"2025-09-07T14:42:36.872521Z","shell.execute_reply.started":"2025-09-07T14:42:36.860429Z","shell.execute_reply":"2025-09-07T14:42:36.871620Z"}},"outputs":[{"name":"stdout","text":"Data preparation functions defined\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(\"Loading datasets...\")\n\n# Start with a smaller subset for testing\nUSE_SMALL_DATASET = True\nMAX_TRAIN_SAMPLES = 5000 if USE_SMALL_DATASET else None\nMAX_VAL_SAMPLES = 500 if USE_SMALL_DATASET else None\n\ntry:\n    train_raw = load_dataset(\n        \"json\",\n        data_files=\"/kaggle/input/stanford-question-answering-dataset/train-v1.1.json\",\n        field=\"data\"\n    )[\"train\"]\n\n    val_raw = load_dataset(\n        \"json\", \n        data_files=\"/kaggle/input/stanford-question-answering-dataset/dev-v1.1.json\",\n        field=\"data\"\n    )[\"train\"]\n    \n    print(f\"Loaded {len(train_raw)} training articles and {len(val_raw)} validation articles\")\n    \nexcept Exception as e:\n    print(f\"Error loading Kaggle datasets: {e}\")\n    print(\"Falling back to HuggingFace SQuAD dataset...\")\n    squad_dataset = load_dataset(\"squad\")\n    \n    if USE_SMALL_DATASET:\n        train_raw = squad_dataset[\"train\"].select(range(min(MAX_TRAIN_SAMPLES, len(squad_dataset[\"train\"]))))\n        val_raw = squad_dataset[\"validation\"].select(range(min(MAX_VAL_SAMPLES, len(squad_dataset[\"validation\"]))))\n    else:\n        train_raw = squad_dataset[\"train\"]\n        val_raw = squad_dataset[\"validation\"]\n\n# Flatten data\nprint(\"Flattening data structure...\")\ntrain_flattened = flatten_squad_data(train_raw, MAX_TRAIN_SAMPLES)\nval_flattened = flatten_squad_data(val_raw, MAX_VAL_SAMPLES)\n\n# Create datasets\nraw_datasets = {\n    \"train\": Dataset.from_dict(train_flattened),\n    \"validation\": Dataset.from_dict(val_flattened)\n}\n\nprint(f\"Using {len(raw_datasets['train'])} training examples and {len(raw_datasets['validation'])} validation examples\")\n\n# Cleanup\ndel train_raw, val_raw, train_flattened, val_flattened\ncleanup_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T14:42:36.873658Z","iopub.execute_input":"2025-09-07T14:42:36.873950Z","iopub.status.idle":"2025-09-07T14:42:37.563462Z","shell.execute_reply.started":"2025-09-07T14:42:36.873925Z","shell.execute_reply":"2025-09-07T14:42:37.562791Z"}},"outputs":[{"name":"stdout","text":"Loading datasets...\nLoaded 442 training articles and 48 validation articles\nFlattening data structure...\nUsing 5000 training examples and 500 validation examples\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"MODEL_NAME = \"distilbert-base-uncased\"\nMAX_LENGTH = 256  # Reduced from 384 to save memory\nDOC_STRIDE = 64   # Reduced from 128\nOUTPUT_DIR = \"./distilbert-squad-iou\"\n\nprint(f\"Loading model and tokenizer: {MODEL_NAME}\")\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\nmodel = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n\n# Setup device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"Total GPU memory: {total_memory:.1f} GB\")\n    \n    # Check initial memory usage\n    cleanup_memory()\n    allocated = torch.cuda.memory_allocated(0) / 1024**3\n    print(f\"Initial GPU memory allocated: {allocated:.2f} GB\")\n\nmodel.to(device)\ncleanup_memory()\n\nprint(\"Model loaded successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T14:42:37.565072Z","iopub.execute_input":"2025-09-07T14:42:37.565355Z","iopub.status.idle":"2025-09-07T14:42:38.878862Z","shell.execute_reply.started":"2025-09-07T14:42:37.565339Z","shell.execute_reply":"2025-09-07T14:42:38.878245Z"}},"outputs":[{"name":"stdout","text":"Loading model and tokenizer: distilbert-base-uncased\nUsing device: cuda\nGPU: Tesla T4\nTotal GPU memory: 14.7 GB\nInitial GPU memory allocated: 0.00 GB\nModel loaded successfully\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(\"Preprocessing training data...\")\n\n# Very conservative batch size for preprocessing\nPREPROCESS_BATCH_SIZE = 100\n\ndef monitor_memory():\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated(0) / 1024**3\n        reserved = torch.cuda.memory_reserved(0) / 1024**3\n        print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n\nmonitor_memory()\n\n# Process training data\ntrain_dataset = raw_datasets[\"train\"].map(\n    lambda examples: prepare_train_features(\n        examples, tokenizer, max_length=MAX_LENGTH, doc_stride=DOC_STRIDE\n    ),\n    batched=True,\n    batch_size=PREPROCESS_BATCH_SIZE,\n    remove_columns=raw_datasets[\"train\"].column_names,\n    desc=\"Processing training data\",\n    writer_batch_size=PREPROCESS_BATCH_SIZE\n)\n\nprint(f\"Training dataset processed: {len(train_dataset)} examples\")\nmonitor_memory()\ncleanup_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T14:42:38.879526Z","iopub.execute_input":"2025-09-07T14:42:38.879738Z","iopub.status.idle":"2025-09-07T14:42:42.590336Z","shell.execute_reply.started":"2025-09-07T14:42:38.879717Z","shell.execute_reply":"2025-09-07T14:42:42.589481Z"}},"outputs":[{"name":"stdout","text":"Preprocessing training data...\nGPU Memory - Allocated: 0.25GB, Reserved: 0.27GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing training data:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4e39534a0b3461d80942cd837437b57"}},"metadata":{}},{"name":"stdout","text":"Training dataset processed: 5765 examples\nGPU Memory - Allocated: 0.25GB, Reserved: 0.27GB\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(\"Setting up training with minimal resource usage...\")\n\n# Very conservative training arguments\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    eval_strategy=\"no\",\n    save_strategy=\"no\",  # Disable saving during training to save space\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    learning_rate=5e-5,\n    per_device_train_batch_size=2,  # Very small batch size\n    gradient_accumulation_steps=8,  # Compensate with gradient accumulation\n    num_train_epochs=1,  # Start with just 1 epoch\n    weight_decay=0.01,\n    warmup_steps=100,\n    max_grad_norm=1.0,\n    fp16=torch.cuda.is_available(),\n    dataloader_num_workers=0,\n    dataloader_pin_memory=False,\n    remove_unused_columns=True,\n    report_to=None,\n    push_to_hub=False,\n    skip_memory_metrics=False,\n    log_level=\"error\",\n)\n\nprint(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    processing_class=tokenizer,\n    data_collator=default_data_collator,\n)\n\nprint(\"Trainer created successfully\")\nmonitor_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T14:42:42.591145Z","iopub.execute_input":"2025-09-07T14:42:42.591334Z","iopub.status.idle":"2025-09-07T14:42:43.486282Z","shell.execute_reply.started":"2025-09-07T14:42:42.591319Z","shell.execute_reply":"2025-09-07T14:42:43.485727Z"}},"outputs":[{"name":"stdout","text":"Setting up training with minimal resource usage...\nEffective batch size: 16\nTrainer created successfully\nGPU Memory - Allocated: 0.25GB, Reserved: 0.27GB\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(\"Running pre-training memory and data checks...\")\n\n# Check if we can load a small batch\ntry:\n    # Test with a tiny batch\n    test_batch = trainer.get_train_dataloader()\n    print(f\"Train dataloader created successfully\")\n    print(f\"Number of batches: {len(test_batch)}\")\n    \n    # Try to get one batch\n    batch_iter = iter(test_batch)\n    first_batch = next(batch_iter)\n    print(f\"First batch keys: {first_batch.keys()}\")\n    print(f\"Batch size: {first_batch['input_ids'].shape[0]}\")\n    print(f\"Sequence length: {first_batch['input_ids'].shape[1]}\")\n    \n    cleanup_memory()\n    monitor_memory()\n    \n    print(\"✓ Data loading test successful\")\n    \nexcept Exception as e:\n    print(f\"✗ Data loading test failed: {e}\")\n    print(\"Try reducing batch size further or using CPU\")\n\n# Test forward pass\ntry:\n    with torch.no_grad():\n        # Move batch to device\n        test_inputs = {k: v.to(device) for k, v in first_batch.items()}\n        outputs = model(**test_inputs)\n        print(f\"✓ Forward pass test successful\")\n        print(f\"Loss: {outputs.loss.item():.4f}\")\n        \n    cleanup_memory()\n    monitor_memory()\n    \nexcept Exception as e:\n    print(f\"✗ Forward pass test failed: {e}\")\n    print(\"Model may be too large for available GPU memory\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T14:42:51.150699Z","iopub.execute_input":"2025-09-07T14:42:51.151749Z","iopub.status.idle":"2025-09-07T14:42:52.130407Z","shell.execute_reply.started":"2025-09-07T14:42:51.151716Z","shell.execute_reply":"2025-09-07T14:42:52.129787Z"}},"outputs":[{"name":"stdout","text":"Running pre-training memory and data checks...\nTrain dataloader created successfully\nNumber of batches: 1442\nFirst batch keys: dict_keys(['input_ids', 'attention_mask', 'start_positions', 'end_positions'])\nBatch size: 4\nSequence length: 256\nGPU Memory - Allocated: 0.25GB, Reserved: 0.27GB\n✓ Data loading test successful\n✓ Forward pass test successful\nLoss: 5.5433\nGPU Memory - Allocated: 0.26GB, Reserved: 0.29GB\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(\"=\"*50)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*50)\n\ntry:\n    # Clear everything before training\n    cleanup_memory()\n    monitor_memory()\n    \n    print(f\"Training on {len(train_dataset)} examples...\")\n    print(f\"This will be approximately {len(trainer.get_train_dataloader())} steps\")\n    \n    # Start training with error recovery\n    trainer.train()\n    \n    print(\"✓ Training completed successfully!\")\n    \nexcept torch.cuda.OutOfMemoryError as e:\n    print(f\"✗ CUDA Out of Memory Error: {e}\")\n    print(\"Solutions:\")\n    print(\"1. Reduce per_device_train_batch_size to 1\")\n    print(\"2. Increase gradient_accumulation_steps\")\n    print(\"3. Reduce MAX_LENGTH further\")\n    print(\"4. Use CPU training (slower but more memory)\")\n    \n    cleanup_memory()\n    \nexcept RuntimeError as e:\n    print(f\"✗ Runtime Error: {e}\")\n    print(\"This might be a CUDA or model loading issue\")\n    cleanup_memory()\n    \nexcept Exception as e:\n    print(f\"✗ Unexpected error: {e}\")\n    print(f\"Error type: {type(e).__name__}\")\n    cleanup_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T14:42:56.654680Z","iopub.execute_input":"2025-09-07T14:42:56.655304Z","iopub.status.idle":"2025-09-07T14:45:36.373256Z","shell.execute_reply.started":"2025-09-07T14:42:56.655271Z","shell.execute_reply":"2025-09-07T14:45:36.372504Z"}},"outputs":[{"name":"stdout","text":"==================================================\nSTARTING TRAINING\n==================================================\nGPU Memory - Allocated: 0.26GB, Reserved: 0.29GB\nTraining on 5765 examples...\nThis will be approximately 1442 steps\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'loss': 4.373, 'grad_norm': 664331.5625, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.5547850208044383}\n{'train_runtime': 158.2547, 'train_samples_per_second': 36.429, 'train_steps_per_second': 1.144, 'train_loss': 3.589718897698334, 'init_mem_cpu_alloc_delta': 262144, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 753188864, 'train_mem_gpu_alloc_delta': 553178112, 'train_mem_cpu_peaked_delta': 53248, 'train_mem_gpu_peaked_delta': 810799104, 'before_init_mem_cpu': 1554771968, 'before_init_mem_gpu': 266594816, 'epoch': 1.0}\n✓ Training completed successfully!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(\"Saving model and cleaning up...\")\n\ntry:\n    # Save the model\n    trainer.save_model(OUTPUT_DIR)\n    tokenizer.save_pretrained(OUTPUT_DIR)\n    print(f\"✓ Model saved to {OUTPUT_DIR}\")\n    \nexcept Exception as e:\n    print(f\"✗ Error saving model: {e}\")\n\n# Final cleanup\ndel trainer, model\ncleanup_memory()\nmonitor_memory()\n\nprint(\"Training session complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T14:47:08.824211Z","iopub.execute_input":"2025-09-07T14:47:08.824943Z","iopub.status.idle":"2025-09-07T14:47:09.873749Z","shell.execute_reply.started":"2025-09-07T14:47:08.824916Z","shell.execute_reply":"2025-09-07T14:47:09.872891Z"}},"outputs":[{"name":"stdout","text":"Saving model and cleaning up...\n✓ Model saved to ./distilbert-squad-iou\nGPU Memory - Allocated: 0.03GB, Reserved: 0.04GB\nTraining session complete!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"print(\"Testing saved model...\")\n\ntry:\n    # Load saved model\n    model = AutoModelForQuestionAnswering.from_pretrained(OUTPUT_DIR)\n    tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n    \n    # Create simple pipeline\n    qa_pipeline = pipeline(\n        \"question-answering\",\n        model=model,\n        tokenizer=tokenizer,\n        device=0 if torch.cuda.is_available() else -1\n    )\n    \n    # Test inference\n    test_context = \"The quick brown fox jumps over the lazy dog. The fox is very clever.\"\n    test_question = \"What jumps over the dog?\"\n    \n    result = qa_pipeline(question=test_question, context=test_context)\n    print(f\"✓ Test successful!\")\n    print(f\"Question: {test_question}\")\n    print(f\"Answer: {result['answer']}\")\n    print(f\"Confidence: {result['score']:.4f}\")\n    \nexcept Exception as e:\n    print(f\"✗ Model testing failed: {e}\")\n\nprint(\"All done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T14:47:12.716131Z","iopub.execute_input":"2025-09-07T14:47:12.716466Z","iopub.status.idle":"2025-09-07T14:47:12.893120Z","shell.execute_reply.started":"2025-09-07T14:47:12.716443Z","shell.execute_reply":"2025-09-07T14:47:12.892270Z"}},"outputs":[{"name":"stdout","text":"Testing saved model...\n✓ Test successful!\nQuestion: What jumps over the dog?\nAnswer: quick brown fox\nConfidence: 0.2650\nAll done!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"CPU FALLBACK TRAINING\")\nprint(\"=\"*30)\nprint(\"Use this only if GPU training failed!\")\n\ndef train_on_cpu():\n    \"\"\"Fallback training on CPU with very small dataset\"\"\"\n    \n    # Use even smaller dataset for CPU\n    small_train = raw_datasets[\"train\"].select(range(100))\n    \n    # Load model on CPU\n    model_cpu = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n    \n    # Process data\n    train_dataset_cpu = small_train.map(\n        lambda examples: prepare_train_features(\n            examples, tokenizer, max_length=128, doc_stride=32\n        ),\n        batched=True,\n        batch_size=50,\n        remove_columns=small_train.column_names,\n    )\n    \n    # CPU training args\n    training_args_cpu = TrainingArguments(\n        output_dir=f\"{OUTPUT_DIR}_cpu\",\n        eval_strategy=\"no\",\n        save_strategy=\"no\",\n        logging_steps=10,\n        learning_rate=5e-5,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        num_train_epochs=1,\n        dataloader_num_workers=0,\n        report_to=None,\n        fp16=False,  # No mixed precision on CPU\n    )\n    \n    trainer_cpu = Trainer(\n        model=model_cpu,\n        args=training_args_cpu,\n        train_dataset=train_dataset_cpu,\n        processing_class=tokenizer,\n        data_collator=default_data_collator,\n    )\n    \n    print(\"Starting CPU training...\")\n    trainer_cpu.train()\n    \n    # Save CPU model\n    trainer_cpu.save_model(f\"{OUTPUT_DIR}_cpu\")\n    print(\"CPU training completed!\")\n\n# Uncomment the next line to run CPU training\n# train_on_cpu()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T14:47:16.483368Z","iopub.execute_input":"2025-09-07T14:47:16.483626Z","iopub.status.idle":"2025-09-07T14:47:16.490601Z","shell.execute_reply.started":"2025-09-07T14:47:16.483609Z","shell.execute_reply":"2025-09-07T14:47:16.489842Z"}},"outputs":[{"name":"stdout","text":"CPU FALLBACK TRAINING\n==============================\nUse this only if GPU training failed!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}