{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# CRITICAL: Set environment variables before importing any other libraries.\n# This prevents deadlocks related to tokenizers and external reporting tools (like W&B).\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:29:47.042069Z","iopub.execute_input":"2025-08-13T05:29:47.042576Z","iopub.status.idle":"2025-08-13T05:29:47.049056Z","shell.execute_reply.started":"2025-08-13T05:29:47.042552Z","shell.execute_reply":"2025-08-13T05:29:47.048515Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade transformers --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:29:47.050251Z","iopub.execute_input":"2025-08-13T05:29:47.050452Z","iopub.status.idle":"2025-08-13T05:30:00.763566Z","shell.execute_reply.started":"2025-08-13T05:29:47.050437Z","shell.execute_reply":"2025-08-13T05:30:00.762830Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# import transformers\n# print(transformers.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:30:00.764400Z","iopub.execute_input":"2025-08-13T05:30:00.764588Z","iopub.status.idle":"2025-08-13T05:30:00.768264Z","shell.execute_reply.started":"2025-08-13T05:30:00.764567Z","shell.execute_reply":"2025-08-13T05:30:00.767517Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# print(transformers.__file__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:30:00.768972Z","iopub.execute_input":"2025-08-13T05:30:00.769199Z","iopub.status.idle":"2025-08-13T05:30:00.785083Z","shell.execute_reply.started":"2025-08-13T05:30:00.769160Z","shell.execute_reply":"2025-08-13T05:30:00.784419Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:30:00.786911Z","iopub.execute_input":"2025-08-13T05:30:00.787074Z","iopub.status.idle":"2025-08-13T05:30:30.857017Z","shell.execute_reply.started":"2025-08-13T05:30:00.787060Z","shell.execute_reply":"2025-08-13T05:30:30.856464Z"}},"outputs":[{"name":"stderr","text":"2025-08-13 05:30:15.044977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755063015.244244      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755063015.305473      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\nprint(transformers.TrainingArguments)\nprint(type(transformers.TrainingArguments))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:30:30.857716Z","iopub.execute_input":"2025-08-13T05:30:30.858288Z","iopub.status.idle":"2025-08-13T05:30:30.862306Z","shell.execute_reply.started":"2025-08-13T05:30:30.858268Z","shell.execute_reply":"2025-08-13T05:30:30.861587Z"}},"outputs":[{"name":"stdout","text":"4.55.0\n<class 'transformers.training_args.TrainingArguments'>\n<class 'type'>\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# from transformers import TrainingArguments\n# print(TrainingArguments)\n# print(type(TrainingArguments))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:30:30.863075Z","iopub.execute_input":"2025-08-13T05:30:30.863415Z","iopub.status.idle":"2025-08-13T05:30:30.936574Z","shell.execute_reply.started":"2025-08-13T05:30:30.863393Z","shell.execute_reply":"2025-08-13T05:30:30.935931Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load CSV\ndf = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n\n# Quick peek\nprint(df.head())\nprint(df['sentiment'].value_counts())\n\n# Map labels to ints (positive=1, negative=0)\nlabel_map = {'positive': 1, 'negative': 0}\ndf['label'] = df['sentiment'].map(label_map)\n\n# Subsample for faster runtime: stratified split to keep label distribution balanced\ntrain_df, test_df = train_test_split(df, train_size=5000, test_size=1000, stratify=df['label'], random_state=42)\n\nprint(f\"Train subset size: {len(train_df)}\")\nprint(f\"Test subset size: {len(test_df)}\")\nprint(train_df['label'].value_counts())\nprint(test_df['label'].value_counts())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:30:30.937349Z","iopub.execute_input":"2025-08-13T05:30:30.937601Z","iopub.status.idle":"2025-08-13T05:30:32.358228Z","shell.execute_reply.started":"2025-08-13T05:30:30.937577Z","shell.execute_reply":"2025-08-13T05:30:32.357602Z"}},"outputs":[{"name":"stdout","text":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive\nsentiment\npositive    25000\nnegative    25000\nName: count, dtype: int64\nTrain subset size: 5000\nTest subset size: 1000\nlabel\n0    2500\n1    2500\nName: count, dtype: int64\nlabel\n1    500\n0    500\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\ntest_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n\nprint(train_dataset)\nprint(test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:30:32.358983Z","iopub.execute_input":"2025-08-13T05:30:32.359235Z","iopub.status.idle":"2025-08-13T05:30:32.435008Z","shell.execute_reply.started":"2025-08-13T05:30:32.359217Z","shell.execute_reply":"2025-08-13T05:30:32.434222Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['review', 'sentiment', 'label'],\n    num_rows: 5000\n})\nDataset({\n    features: ['review', 'sentiment', 'label'],\n    num_rows: 1000\n})\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_checkpoint = \"bert-base-uncased\"  # You can change this later for other models\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\ndef tokenize_function(example):\n    return tokenizer(example['review'], padding=\"max_length\", truncation=True, max_length=512)\n\ntrain_tokenized = train_dataset.map(tokenize_function, batched=True)\ntest_tokenized = test_dataset.map(tokenize_function, batched=True)\n\n# Set format for PyTorch\ntrain_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\ntest_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:30:32.436040Z","iopub.execute_input":"2025-08-13T05:30:32.436508Z","iopub.status.idle":"2025-08-13T05:30:39.260537Z","shell.execute_reply.started":"2025-08-13T05:30:32.436485Z","shell.execute_reply":"2025-08-13T05:30:39.259942Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3304b7d186f8494080cbc7ff1a151be5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8c68fb4d6e84725ae5d295915e66e1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa289560a79949dc976e70eeff229666"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfd1bd5dd9a345158311a89ae30c9af2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f319180078604856998b67fc2a8f35ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92c12a75f70949319752e8b28a805f36"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Prepare dataset for the Trainer by renaming the 'label' column to 'labels'\n# and removing columns that are no longer needed.\ntrain_tokenized = train_tokenized.rename_column(\"label\", \"labels\")\ntest_tokenized = test_tokenized.rename_column(\"label\", \"labels\")\ntrain_tokenized = train_tokenized.remove_columns(['review', 'sentiment'])\ntest_tokenized = test_tokenized.remove_columns(['review', 'sentiment'])\n\n# Set the format to PyTorch tensors for the Trainer\ntrain_tokenized.set_format('torch')\ntest_tokenized.set_format('torch')\n\nprint(\"Processed training dataset features:\")\nprint(train_tokenized)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:30:39.261307Z","iopub.execute_input":"2025-08-13T05:30:39.261567Z","iopub.status.idle":"2025-08-13T05:30:39.271881Z","shell.execute_reply.started":"2025-08-13T05:30:39.261539Z","shell.execute_reply":"2025-08-13T05:30:39.271381Z"}},"outputs":[{"name":"stdout","text":"Processed training dataset features:\nDataset({\n    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 5000\n})\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import f1_score, accuracy_score\nimport gc\n\n# Free up memory before starting training\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# Load the pre-trained model\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n\n# Define the function to compute metrics during evaluation\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average='weighted')\n    return {\"accuracy\": acc, \"f1\": f1}\n\n# Define training arguments with settings safe for notebooks\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=1,\n    learning_rate=2e-5,\n    \n    # Batch sizes and gradient accumulation\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2, # Effective batch size = 4 * 2 = 8\n    \n    # Evaluation and logging strategies\n    eval_strategy=\"steps\",\n    eval_steps=250,      # Evaluate periodically\n    logging_steps=100,   # Log training loss periodically\n    \n    # Key settings to prevent hangs and optimize performance\n    fp16=True,                           # Use mixed-precision for faster training on compatible GPUs\n    dataloader_num_workers=0,            # CRITICAL: Must be 0 in Kaggle notebooks\n    report_to=[],                        # Disables external reporting\n    save_strategy=\"no\",                  # Do not save model checkpoints during training\n    load_best_model_at_end=False,\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tokenized, # Use the cleaned dataset from the previous cell\n    eval_dataset=test_tokenized,   # Use the cleaned dataset from the previous cell\n    compute_metrics=compute_metrics,\n)\n\n# Start training!\nprint(\"🚀 Starting training...\")\ntrainer.train()\n\n# Run evaluation on the test set\nprint(\"\\n✅ Training complete. Running evaluation...\")\neval_results = trainer.evaluate()\nprint(f\"\\nEvaluation results: {eval_results}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:30:39.272611Z","iopub.execute_input":"2025-08-13T05:30:39.272832Z","iopub.status.idle":"2025-08-13T05:36:39.498662Z","shell.execute_reply.started":"2025-08-13T05:30:39.272816Z","shell.execute_reply":"2025-08-13T05:36:39.497828Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d72fb19188d4664a8b7f89b81372c0d"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"🚀 Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 05:35, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>0.278700</td>\n      <td>0.244984</td>\n      <td>0.917000</td>\n      <td>0.916939</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n✅ Training complete. Running evaluation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nEvaluation results: {'eval_loss': 0.2164091020822525, 'eval_accuracy': 0.919, 'eval_f1': 0.9189934384685159, 'eval_runtime': 18.9399, 'eval_samples_per_second': 52.799, 'eval_steps_per_second': 3.326, 'epoch': 1.0}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# 1. Install and Set Environment\n# -------------------------------\n!pip install --upgrade transformers datasets accelerate -q\n\nimport os\n# This prevents deadlocks in Kaggle notebooks\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# 2. Imports\n# -----------\nimport pandas as pd\nimport numpy as np\nimport torch\nimport gc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    pipeline\n)\n\n# 3. Load Full Dataset\n# --------------------\nprint(\"Loading the full IMDB dataset...\")\ndf = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n\n# Map labels to integers\nlabel_map = {'positive': 1, 'negative': 0}\ndf['label'] = df['sentiment'].map(label_map)\n\n# Split into training and testing sets (full size)\ntrain_df, test_df = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df['label'],\n    random_state=42\n)\n\nprint(f\"Full training set size: {len(train_df)}\")\nprint(f\"Full test set size: {len(test_df)}\")\n\n# Create a smaller subset for quick model comparison\nsub_train_df, _ = train_test_split(\n    train_df,\n    train_size=5000,\n    stratify=train_df['label'],\n    random_state=42\n)\nsub_test_df, _ = train_test_split(\n    test_df,\n    test_size=1000,\n    stratify=test_df['label'],\n    random_state=42\n)\n\nprint(f\"\\nSubset training size for comparison: {len(sub_train_df)}\")\nprint(f\"Subset testing size for comparison: {len(sub_test_df)}\")\n\n# Convert to Hugging Face Dataset objects\ntrain_dataset_full = Dataset.from_pandas(train_df.reset_index(drop=True))\ntest_dataset_full = Dataset.from_pandas(test_df.reset_index(drop=True))\n\ntrain_dataset_sub = Dataset.from_pandas(sub_train_df.reset_index(drop=True))\ntest_dataset_sub = Dataset.from_pandas(sub_test_df.reset_index(drop=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:36:39.499573Z","iopub.execute_input":"2025-08-13T05:36:39.499848Z","iopub.status.idle":"2025-08-13T05:38:06.969692Z","shell.execute_reply.started":"2025-08-13T05:36:39.499822Z","shell.execute_reply":"2025-08-13T05:38:06.969032Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.7/374.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mLoading the full IMDB dataset...\nFull training set size: 40000\nFull test set size: 10000\n\nSubset training size for comparison: 5000\nSubset testing size for comparison: 9000\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# 1. Define Models and Evaluation Function\n# ----------------------------------------\nmodel_checkpoints = {\n    \"DistilBERT\": \"distilbert-base-uncased\",\n    \"ELECTRA\": \"google/electra-small-discriminator\",\n    \"DeBERTa-v3\": \"microsoft/deberta-v3-base\", # Substitute for \"ModernBERT\"\n    \"ALBERT\": \"albert-base-v2\",               # Substitute for \"Ettin\"\n    \"GTE\": \"thenlper/gte-base\"                # As requested. Note: GTE is primarily for embeddings.\n}\n\n# This is our custom F1 score function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    f1 = f1_score(labels, predictions, average='weighted')\n    return {\"f1\": f1}\n\n# 2. Loop Through Models for Fine-tuning\n# --------------------------------------\nresults = {}\n\nfor name, checkpoint in model_checkpoints.items():\n    print(f\"--- Starting fine-tuning for {name} ({checkpoint}) ---\")\n    \n    # a. Tokenize Data\n    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n    \n    def tokenize_function(examples):\n        return tokenizer(examples['review'], padding=\"max_length\", truncation=True, max_length=512)\n        \n    train_tokenized = train_dataset_sub.map(tokenize_function, batched=True)\n    test_tokenized = test_dataset_sub.map(tokenize_function, batched=True)\n\n    # b. Prepare Dataset for Trainer (Corrected Line)\n    train_processed = train_tokenized.rename_column(\"label\", \"labels\").remove_columns(['review', 'sentiment'])\n    test_processed = test_tokenized.rename_column(\"label\", \"labels\").remove_columns(['review', 'sentiment'])\n    train_processed.set_format('torch')\n    test_processed.set_format('torch')\n\n    # c. Setup Model and Trainer\n    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n    \n    training_args = TrainingArguments(\n        output_dir=f'./results_{name}',\n        num_train_epochs=1,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=16,\n        learning_rate=2e-5,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        fp16=True,\n        dataloader_num_workers=0,\n        logging_steps=100,\n        report_to=[],\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_processed,\n        eval_dataset=test_processed,\n        compute_metrics=compute_metrics,\n    )\n    \n    # d. Train and Evaluate\n    trainer.train()\n    eval_result = trainer.evaluate()\n    results[name] = eval_result['eval_f1']\n    \n    print(f\"--- F1 Score for {name}: {eval_result['eval_f1']:.4f} ---\")\n    \n    # e. Clean up memory\n    del model, trainer, tokenizer\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# 3. Find and Display the Best Model\n# -----------------------------------\nprint(\"\\n--- Model Comparison Results ---\")\nfor name, f1 in results.items():\n    print(f\"{name}: {f1:.4f}\")\n\nbest_model_name = max(results, key=results.get)\nbest_model_checkpoint = model_checkpoints[best_model_name]\nprint(f\"\\n🏆 Best performing model on the subset: {best_model_name} ({best_model_checkpoint}) with F1 score: {results[best_model_name]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T05:38:06.972221Z","iopub.execute_input":"2025-08-13T05:38:06.972455Z","iopub.status.idle":"2025-08-13T06:24:49.073969Z","shell.execute_reply.started":"2025-08-13T05:38:06.972431Z","shell.execute_reply":"2025-08-13T06:24:49.073325Z"}},"outputs":[{"name":"stdout","text":"--- Starting fine-tuning for DistilBERT (distilbert-base-uncased) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f200d8e1b9644536ac98a8336039198f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f882b323dcd44443a441d015c77fc519"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa2309a6d86c478081b60870e25bdda2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c1f372079b7426f8dce6a00bbf351b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"613d53cd21b34d0d968ece80debc1170"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f860e89387c2443abcbdd25bea7a7863"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee3bda776094908bbcb3aeec559e828"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 03:56, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.240200</td>\n      <td>0.255422</td>\n      <td>0.898427</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 01:24]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"--- F1 Score for DistilBERT: 0.8984 ---\n--- Starting fine-tuning for ELECTRA (google/electra-small-discriminator) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4774f13281f4a22b1e77cb8f5450b53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6353d2efa824ac6a7d57565ce605f1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c8bcfb212fe49fdb4dc03a48bbfeffe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9adb327db2674d9398538dc5ed52bbb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d357bfa28a944359709bfb249c4e0b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f6a76293d404beeb2ba45e85594ed3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/54.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f79817a176a14d759ff3242c80bedf5e"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/54.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa35c997bd9b47ba88316defa9aa81aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 01:30, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.490200</td>\n      <td>0.473122</td>\n      <td>0.834295</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 00:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"--- F1 Score for ELECTRA: 0.8343 ---\n--- Starting fine-tuning for DeBERTa-v3 (microsoft/deberta-v3-base) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afb4ca3a73874dab95d6e863800073ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eae2f0c560945178037b39427bbd9cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5379d381da849ffa760245c72217e76"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fe45e5bdeb842969a02db8eb153b60a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ca057807f294d218416e47e822c244a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4035f26913714cf0bbba913f2cfb3eb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dfaabdaf7934a2b9d30a51580cce5a1"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 11:37, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.187700</td>\n      <td>0.207711</td>\n      <td>0.945439</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 04:28]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"--- F1 Score for DeBERTa-v3: 0.9454 ---\n--- Starting fine-tuning for ALBERT (albert-base-v2) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8827a98dcf00474c8d45c61d0c9d3357"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbfae4f9c2834ccfa573b7ef0b2b0c34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dbf067d8c69462ba0f497e30dce480b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79da8edfccba4d218a9fa98c7cf20c79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4809f8755384f92acad0ba52e4fbddc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa4530518109496894003bc677d0942e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3373b3cbf15a49a182096e6ac2f57a3e"}},"metadata":{}},{"name":"stderr","text":"Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 07:44, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.249700</td>\n      <td>0.225378</td>\n      <td>0.916566</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 03:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"--- F1 Score for ALBERT: 0.9166 ---\n--- Starting fine-tuning for GTE (thenlper/gte-base) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0249bb7128b043848a1dc07599d6af54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46e9c153304a4390b4265dc1f74722d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"833b9c6a3f9b48df8844b68869eb2912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3159e339dbd4692a869cff899751fb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a10d5f2cd604ba88add3acac2d447a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80883a9406a34381befa2fc0e7a69a9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/618 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d5e3a6ca27b4facacf5039371cd8012"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/219M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1db7c22168b427fb1c5828feb513f74"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at thenlper/gte-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 07:49, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.212200</td>\n      <td>0.205423</td>\n      <td>0.924111</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [282/282 02:50]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"--- F1 Score for GTE: 0.9241 ---\n\n--- Model Comparison Results ---\nDistilBERT: 0.8984\nELECTRA: 0.8343\nDeBERTa-v3: 0.9454\nALBERT: 0.9166\nGTE: 0.9241\n\n🏆 Best performing model on the subset: DeBERTa-v3 (microsoft/deberta-v3-base) with F1 score: 0.9454\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(f\"--- Starting FULL fine-tuning for the best model: {best_model_name} ---\")\n\n# 1. Tokenize the FULL dataset\n# -----------------------------\ntokenizer = AutoTokenizer.from_pretrained(best_model_checkpoint)\n\ndef tokenize_function(examples):\n    return tokenizer(examples['review'], padding=\"max_length\", truncation=True, max_length=512)\n\ntrain_tokenized_full = train_dataset_full.map(tokenize_function, batched=True, remove_columns=['review', 'sentiment'])\ntest_tokenized_full = test_dataset_full.map(tokenize_function, batched=True, remove_columns=['review', 'sentiment'])\n\n# 2. Prepare datasets for the Trainer\n# -----------------------------------\ntrain_processed_full = train_tokenized_full.rename_column(\"label\", \"labels\")\ntest_processed_full = test_tokenized_full.rename_column(\"label\", \"labels\")\ntrain_processed_full.set_format('torch')\ntest_processed_full.set_format('torch')\n\n# 3. Setup Model and Training Arguments for the full run\n# ------------------------------------------------------\nmodel = AutoModelForSequenceClassification.from_pretrained(best_model_checkpoint, num_labels=2)\n\n# Adjust arguments for the larger dataset\ntraining_args_full = TrainingArguments(\n    output_dir=f'./results_full_{best_model_name}',\n    num_train_epochs=1, # One epoch is often enough for large datasets\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    fp16=True,\n    dataloader_num_workers=0,\n    report_to=[],\n)\n\ntrainer_full = Trainer(\n    model=model,\n    args=training_args_full,\n    train_dataset=train_processed_full,\n    eval_dataset=test_processed_full,\n    compute_metrics=compute_metrics,\n)\n\n# 4. Train and Evaluate on the full dataset\n# -----------------------------------------\ntrainer_full.train()\nfinal_eval_results = trainer_full.evaluate()\n\nprint(\"\\n--- Final Model Evaluation (on full test set) ---\")\nprint(f\"Accuracy: {final_eval_results['eval_accuracy']:.4f}\")\nprint(f\"F1 Score: {final_eval_results['eval_f1']:.4f}\")\n\n# Save the final model and tokenizer\nfinal_model_path = f\"./final_model_{best_model_name}\"\ntrainer_full.save_model(final_model_path)\ntokenizer.save_pretrained(final_model_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Running Inference on 10 Random Samples ---\")\n\n# 1. Load the fine-tuned model using a pipeline\n# ----------------------------------------------\n# The pipeline automatically handles tokenization, model loading, and output formatting.\nfinal_model_path = f\"./final_model_{best_model_name}\"\nclassifier = pipeline(\"text-classification\", model=final_model_path, device=0) # Use 0 for GPU\n\n# 2. Sample 10 reviews from the original test dataframe\n# -----------------------------------------------------\nsample_reviews = test_df.sample(10, random_state=42)\nreviews_list = sample_reviews['review'].tolist()\nground_truth_labels = sample_reviews['sentiment'].tolist()\n\n# 3. Run predictions\n# ------------------\npredictions = classifier(reviews_list)\n\n# 4. Display results\n# ------------------\nlabel_to_sentiment = { \"LABEL_1\": \"positive\", \"LABEL_0\": \"negative\" }\n\nfor i in range(10):\n    review = reviews_list[i]\n    true_label = ground_truth_labels[i]\n    pred_label_str = predictions[i]['label']\n    pred_sentiment = label_to_sentiment[pred_label_str]\n    pred_score = predictions[i]['score']\n    \n    print(f\"\\n--- Review #{i+1} ---\")\n    print(f\"Review: {review[:300]}...\") # Print first 300 characters\n    print(f\"✅ Ground Truth: {true_label}\")\n    print(f\"🤖 Prediction: {pred_sentiment} (Score: {pred_score:.4f})\")\n    if true_label == pred_sentiment:\n        print(\"Correct! 👍\")\n    else:\n        print(\"Incorrect. 👎\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}