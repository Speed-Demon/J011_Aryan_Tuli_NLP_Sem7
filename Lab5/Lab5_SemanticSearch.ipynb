{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6277,"databundleVersionId":323734,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Disable wandb completely\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n\nimport zipfile\nimport pandas as pd\nimport numpy as np\nfrom sklearn import model_selection\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.linear_model import LogisticRegression\nfrom datasets import Dataset\nimport torch\nfrom sentence_transformers import SentenceTransformer, losses, InputExample\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\nfrom sentence_transformers.similarity_functions import SimilarityFunction\nfrom sentence_transformers.cross_encoder import CrossEncoder\nfrom sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration\nconfig = {\n    \"model_path\": \"microsoft/xtremedistil-l6-h256-uncased\",\n    \"learning_rate\": 5e-4,\n    \"train_batch_size\": 32,  # Reduced for cross-encoder\n    \"eval_batch_size\": 64,\n    \"epochs\": 3,\n    \"warmup_ratio\": 0.1,\n    \"output_dir\": \"./models/\"\n}\n\nprint(\"Loading and preprocessing data...\")\n\n# Extract and load data\nwith zipfile.ZipFile(\"/kaggle/input/quora-question-pairs/train.csv.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"./train/\")\n\n# Also extract test data if needed\ntry:\n    with zipfile.ZipFile(\"/kaggle/input/quora-question-pairs/test.csv.zip\", 'r') as zip_ref:\n        zip_ref.extractall(\"./test/\")\nexcept:\n    print(\"Test file not found or already extracted\")\n\ndf = pd.read_csv(\"./train/train.csv\")\ndf = df.dropna()\ndf = df.rename(columns={'is_duplicate': \"label\"})\ndf = df[[\"question1\", \"question2\", \"label\"]]\n\nprint(f\"Total samples: {len(df)}\")\nprint(f\"Duplicate pairs: {df['label'].sum()}\")\nprint(f\"Non-duplicate pairs: {len(df) - df['label'].sum()}\")\n\n# Create stratified splits\n# First split: train+val (80%) and test (20%)\ntrain_val, test_df = model_selection.train_test_split(\n    df, test_size=0.2, random_state=42, stratify=df[\"label\"]\n)\n\n# Second split: train (64%) and val (16%) from the remaining 80%\ntrain_df, val_df = model_selection.train_test_split(\n    train_val, test_size=0.2, random_state=42, stratify=train_val[\"label\"]\n)\n\nprint(f\"Train samples: {len(train_df)}\")\nprint(f\"Validation samples: {len(val_df)}\")\nprint(f\"Test samples: {len(test_df)}\")\n\n# Convert to datasets\ntrain_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\nval_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\ntest_ds = Dataset.from_pandas(test_df.reset_index(drop=True))\n\ndef evaluate_model_performance(model, test_dataset, model_type=\"bi-encoder\", threshold=0.5):\n    \"\"\"\n    Evaluate model performance using F1-score, precision, recall, and accuracy\n    \"\"\"\n    if model_type == \"cross-encoder\":\n        # For cross-encoder, we can directly get predictions\n        predictions = []\n        true_labels = []\n        \n        for i in range(len(test_dataset)):\n            q1 = test_dataset[i][\"question1\"]\n            q2 = test_dataset[i][\"question2\"]\n            label = test_dataset[i][\"label\"]\n            \n            score = model.predict([q1, q2])[0]\n            predictions.append(1 if score > threshold else 0)\n            true_labels.append(label)\n    else:\n        # For bi-encoder, compute embeddings and cosine similarity\n        embeddings1 = model.encode(test_dataset[\"question1\"], batch_size=config[\"eval_batch_size\"])\n        embeddings2 = model.encode(test_dataset[\"question2\"], batch_size=config[\"eval_batch_size\"])\n        \n        # Compute cosine similarities\n        similarities = np.array([\n            cosine_similarity([emb1], [emb2])[0][0] \n            for emb1, emb2 in zip(embeddings1, embeddings2)\n        ])\n        \n        predictions = (similarities > threshold).astype(int)\n        true_labels = test_dataset[\"label\"]\n    \n    # Calculate metrics\n    f1 = f1_score(true_labels, predictions)\n    precision = precision_score(true_labels, predictions)\n    recall = recall_score(true_labels, predictions)\n    accuracy = accuracy_score(true_labels, predictions)\n    \n    return {\n        \"f1_score\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"accuracy\": accuracy,\n        \"threshold\": threshold\n    }\n\ndef find_optimal_threshold(model, val_dataset, model_type=\"bi-encoder\"):\n    \"\"\"\n    Find optimal threshold for classification\n    \"\"\"\n    thresholds = np.arange(0.1, 0.9, 0.1)\n    best_f1 = 0\n    best_threshold = 0.5\n    \n    for threshold in thresholds:\n        metrics = evaluate_model_performance(model, val_dataset, model_type, threshold)\n        if metrics[\"f1_score\"] > best_f1:\n            best_f1 = metrics[\"f1_score\"]\n            best_threshold = threshold\n    \n    return best_threshold, best_f1\n\n# Results storage\nresults = {}\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXPERIMENT 1: BENCHMARK WITH DEFAULT WEIGHTS\")\nprint(\"=\"*60)\n\n# Load pre-trained model\nmodel_default = SentenceTransformer(config[\"model_path\"])\n\n# Find optimal threshold\nopt_threshold, val_f1 = find_optimal_threshold(model_default, val_ds)\nprint(f\"Optimal threshold: {opt_threshold:.2f} (Val F1: {val_f1:.4f})\")\n\n# Evaluate on test set\nmetrics_default = evaluate_model_performance(model_default, test_ds, threshold=opt_threshold)\nresults[\"Benchmark\"] = metrics_default\nprint(f\"Test F1-Score: {metrics_default['f1_score']:.4f}\")\nprint(f\"Test Precision: {metrics_default['precision']:.4f}\")\nprint(f\"Test Recall: {metrics_default['recall']:.4f}\")\nprint(f\"Test Accuracy: {metrics_default['accuracy']:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXPERIMENT 2: BI-ENCODER WITH COSINE SIMILARITY LOSS\")\nprint(\"=\"*60)\n\nmodel_cosine = SentenceTransformer(config[\"model_path\"])\ntrain_loss_cosine = losses.CosineSimilarityLoss(model=model_cosine)\n\n# Setup evaluator\ndev_evaluator = EmbeddingSimilarityEvaluator(\n    sentences1=val_ds[\"question1\"],\n    sentences2=val_ds[\"question2\"],\n    scores=val_ds[\"label\"],\n    main_similarity=SimilarityFunction.COSINE,\n    name=\"dev-score\",\n)\n\n# Prepare training examples for cosine similarity loss\ntrain_examples = []\nfor i in range(len(train_ds)):\n    train_examples.append(InputExample(\n        texts=[train_ds[i][\"question1\"], train_ds[i][\"question2\"]], \n        label=float(train_ds[i][\"label\"])\n    ))\n\n# Use the fit method instead of trainer\nmodel_cosine.fit(\n    train_objectives=[(torch.utils.data.DataLoader(train_examples, shuffle=True, batch_size=config[\"train_batch_size\"]), train_loss_cosine)],\n    evaluator=dev_evaluator,\n    epochs=config[\"epochs\"],\n    evaluation_steps=1000,\n    warmup_steps=int(len(train_examples) * config[\"epochs\"] * config[\"warmup_ratio\"] / config[\"train_batch_size\"]),\n    output_path=config[\"output_dir\"] + \"cosine/\",\n    save_best_model=True,\n    optimizer_params={'lr': config[\"learning_rate\"]},\n)\n\n# Evaluate\nopt_threshold, val_f1 = find_optimal_threshold(model_cosine, val_ds)\nprint(f\"Optimal threshold: {opt_threshold:.2f} (Val F1: {val_f1:.4f})\")\n\nmetrics_cosine = evaluate_model_performance(model_cosine, test_ds, threshold=opt_threshold)\nresults[\"Cosine Similarity\"] = metrics_cosine\nprint(f\"Test F1-Score: {metrics_cosine['f1_score']:.4f}\")\nprint(f\"Test Precision: {metrics_cosine['precision']:.4f}\")\nprint(f\"Test Recall: {metrics_cosine['recall']:.4f}\")\nprint(f\"Test Accuracy: {metrics_cosine['accuracy']:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXPERIMENT 3: BI-ENCODER WITH CONTRASTIVE LOSS\")\nprint(\"=\"*60)\n\nmodel_contrastive = SentenceTransformer(config[\"model_path\"])\n\n# Prepare training examples for contrastive loss\ntrain_examples = []\nfor i in range(len(train_ds)):\n    train_examples.append(InputExample(\n        texts=[train_ds[i][\"question1\"], train_ds[i][\"question2\"]], \n        label=float(train_ds[i][\"label\"])\n    ))\n\n# Use ContrastiveLoss\ntrain_loss_contrastive = losses.ContrastiveLoss(model=model_contrastive)\n\n# Use the fit method\nmodel_contrastive.fit(\n    train_objectives=[(torch.utils.data.DataLoader(train_examples, shuffle=True, batch_size=config[\"train_batch_size\"]), train_loss_contrastive)],\n    evaluator=dev_evaluator,\n    epochs=config[\"epochs\"],\n    evaluation_steps=1000,\n    warmup_steps=int(len(train_examples) * config[\"epochs\"] * config[\"warmup_ratio\"] / config[\"train_batch_size\"]),\n    output_path=config[\"output_dir\"] + \"contrastive/\",\n    save_best_model=True,\n    optimizer_params={'lr': config[\"learning_rate\"]},\n)\n\n# Evaluate\nopt_threshold, val_f1 = find_optimal_threshold(model_contrastive, val_ds)\nprint(f\"Optimal threshold: {opt_threshold:.2f} (Val F1: {val_f1:.4f})\")\n\nmetrics_contrastive = evaluate_model_performance(model_contrastive, test_ds, threshold=opt_threshold)\nresults[\"Contrastive Loss\"] = metrics_contrastive\nprint(f\"Test F1-Score: {metrics_contrastive['f1_score']:.4f}\")\nprint(f\"Test Precision: {metrics_contrastive['precision']:.4f}\")\nprint(f\"Test Recall: {metrics_contrastive['recall']:.4f}\")\nprint(f\"Test Accuracy: {metrics_contrastive['accuracy']:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXPERIMENT 4: BI-ENCODER WITH MULTIPLE NEGATIVE RANKING LOSS\")\nprint(\"=\"*60)\n\nmodel_mnr = SentenceTransformer(config[\"model_path\"])\n\n# Multiple Negative Ranking Loss\ntrain_loss_mnr = losses.MultipleNegativesRankingLoss(model=model_mnr)\n\n# Prepare training examples - only positive pairs for MNR loss\ntrain_examples_mnr = []\npositive_pairs = train_df[train_df[\"label\"] == 1]\nfor _, row in positive_pairs.iterrows():\n    train_examples_mnr.append(InputExample(\n        texts=[row[\"question1\"], row[\"question2\"]]\n    ))\n\nprint(f\"Using {len(train_examples_mnr)} positive pairs for MNR training\")\n\n# Use the fit method\nmodel_mnr.fit(\n    train_objectives=[(torch.utils.data.DataLoader(train_examples_mnr, shuffle=True, batch_size=config[\"train_batch_size\"]), train_loss_mnr)],\n    evaluator=dev_evaluator,\n    epochs=config[\"epochs\"],\n    evaluation_steps=1000,\n    warmup_steps=int(len(train_examples_mnr) * config[\"epochs\"] * config[\"warmup_ratio\"] / config[\"train_batch_size\"]),\n    output_path=config[\"output_dir\"] + \"mnr/\",\n    save_best_model=True,\n    optimizer_params={'lr': config[\"learning_rate\"]},\n)\n\n# Evaluate\nopt_threshold, val_f1 = find_optimal_threshold(model_mnr, val_ds)\nprint(f\"Optimal threshold: {opt_threshold:.2f} (Val F1: {val_f1:.4f})\")\n\nmetrics_mnr = evaluate_model_performance(model_mnr, test_ds, threshold=opt_threshold)\nresults[\"MNR Loss\"] = metrics_mnr\nprint(f\"Test F1-Score: {metrics_mnr['f1_score']:.4f}\")\nprint(f\"Test Precision: {metrics_mnr['precision']:.4f}\")\nprint(f\"Test Recall: {metrics_mnr['recall']:.4f}\")\nprint(f\"Test Accuracy: {metrics_mnr['accuracy']:.4f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXPERIMENT 5: CROSS-ENCODER\")\nprint(\"=\"*60)\n\n# Initialize cross-encoder\ncross_encoder = CrossEncoder(config[\"model_path\"], num_labels=1)\n\n# Prepare training data for cross-encoder\ntrain_samples = []\nfor i in range(len(train_ds)):\n    train_samples.append(InputExample(\n        texts=[train_ds[i][\"question1\"], train_ds[i][\"question2\"]], \n        label=train_ds[i][\"label\"]\n    ))\n\n# Prepare validation data\nval_samples = []\nfor i in range(len(val_ds)):\n    val_samples.append(InputExample(\n        texts=[val_ds[i][\"question1\"], val_ds[i][\"question2\"]], \n        label=val_ds[i][\"label\"]\n    ))\n\n# Setup evaluator for cross-encoder\nce_evaluator = CEBinaryClassificationEvaluator.from_input_examples(\n    val_samples, name='dev'\n)\n\n# Train cross-encoder using the fit method\ncross_encoder.fit(\n    train_dataloader=torch.utils.data.DataLoader(train_samples, shuffle=True, batch_size=config[\"train_batch_size\"]),\n    evaluator=ce_evaluator,\n    epochs=config[\"epochs\"],\n    evaluation_steps=1000,\n    warmup_steps=int(len(train_samples) * config[\"epochs\"] * config[\"warmup_ratio\"] / config[\"train_batch_size\"]),\n    output_path=config[\"output_dir\"] + \"cross_encoder/\",\n    save_best_model=True,\n    optimizer_params={'lr': config[\"learning_rate\"]},\n)\n\n# Evaluate cross-encoder\nopt_threshold, val_f1 = find_optimal_threshold(cross_encoder, val_ds, model_type=\"cross-encoder\")\nprint(f\"Optimal threshold: {opt_threshold:.2f} (Val F1: {val_f1:.4f})\")\n\nmetrics_cross = evaluate_model_performance(cross_encoder, test_ds, model_type=\"cross-encoder\", threshold=opt_threshold)\nresults[\"Cross-Encoder\"] = metrics_cross\nprint(f\"Test F1-Score: {metrics_cross['f1_score']:.4f}\")\nprint(f\"Test Precision: {metrics_cross['precision']:.4f}\")\nprint(f\"Test Recall: {metrics_cross['recall']:.4f}\")\nprint(f\"Test Accuracy: {metrics_cross['accuracy']:.4f}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL RESULTS COMPARISON\")\nprint(\"=\"*80)\n\n# Create results DataFrame\nresults_df = pd.DataFrame(results).T\nresults_df = results_df.round(4)\nprint(results_df)\n\n# Find best model\nbest_model = results_df['f1_score'].idxmax()\nbest_f1 = results_df['f1_score'].max()\nprint(f\"\\nBest performing model: {best_model}\")\nprint(f\"Best F1-Score: {best_f1:.4f}\")\n\n# Print detailed comparison\nprint(\"\\n\" + \"-\"*50)\nprint(\"DETAILED F1-SCORE COMPARISON:\")\nprint(\"-\"*50)\nfor model_name, metrics in results.items():\n    print(f\"{model_name:20}: {metrics['f1_score']:.4f}\")\n\nprint(\"\\nExperiment completed successfully!\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CREATING PREDICTIONS FOR OFFICIAL TEST SET\")\nprint(\"=\"*60)\n\n# Load official test set (without labels)\ntry:\n    official_test_df = pd.read_csv(\"./test/test.csv\")\n    official_test_df = official_test_df.dropna()\n\n    print(f\"Official test samples: {len(official_test_df)}\")\n\n    # Use the best performing model to create predictions\n    best_model_name = results_df['f1_score'].idxmax()\n    print(f\"Using best model: {best_model_name}\")\n\n    # Get the best model and its threshold\n    if best_model_name == \"Benchmark\":\n        best_model = model_default\n        model_type = \"bi-encoder\"\n    elif best_model_name == \"Cosine Similarity\":\n        best_model = model_cosine\n        model_type = \"bi-encoder\"\n    elif best_model_name == \"Contrastive Loss\":\n        best_model = model_contrastive\n        model_type = \"bi-encoder\"\n    elif best_model_name == \"MNR Loss\":\n        best_model = model_mnr\n        model_type = \"bi-encoder\"\n    else:  # Cross-Encoder\n        best_model = cross_encoder\n        model_type = \"cross-encoder\"\n\n    best_threshold = results[best_model_name][\"threshold\"]\n\n    # Create predictions for official test set\n    print(\"Generating predictions...\")\n    if model_type == \"cross-encoder\":\n        predictions = []\n        for i in range(len(official_test_df)):\n            if i % 1000 == 0:\n                print(f\"Processed {i}/{len(official_test_df)} samples\")\n            \n            q1 = official_test_df.iloc[i][\"question1\"]\n            q2 = official_test_df.iloc[i][\"question2\"]\n            score = best_model.predict([q1, q2])[0]\n            predictions.append(1 if score > best_threshold else 0)\n    else:\n        # Bi-encoder approach\n        embeddings1 = best_model.encode(\n            official_test_df[\"question1\"].tolist(), \n            batch_size=config[\"eval_batch_size\"],\n            show_progress_bar=True\n        )\n        embeddings2 = best_model.encode(\n            official_test_df[\"question2\"].tolist(), \n            batch_size=config[\"eval_batch_size\"],\n            show_progress_bar=True\n        )\n        \n        # Compute similarities\n        similarities = np.array([\n            cosine_similarity([emb1], [emb2])[0][0] \n            for emb1, emb2 in zip(embeddings1, embeddings2)\n        ])\n        \n        predictions = (similarities > best_threshold).astype(int)\n\n    # Create submission file\n    submission_df = pd.DataFrame({\n        'test_id': official_test_df['test_id'],\n        'is_duplicate': predictions\n    })\n\n    submission_df.to_csv('submission.csv', index=False)\n    print(f\"Predictions saved to 'submission.csv'\")\n    print(f\"Predicted duplicates: {sum(predictions)}\")\n    print(f\"Predicted non-duplicates: {len(predictions) - sum(predictions)}\")\n    print(f\"Duplicate ratio: {sum(predictions) / len(predictions):.3f}\")\n\nexcept FileNotFoundError:\n    print(\"Official test file not found. Skipping prediction generation.\")\n\nprint(\"\\nAll experiments completed successfully!\")\nprint(\"Files created:\")\nprint(\"- submission.csv (for Kaggle submission, if test data available)\")\nprint(\"- Model checkpoints in ./models/ directory\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-06T20:48:04.735168Z","iopub.execute_input":"2025-09-06T20:48:04.735882Z","iopub.status.idle":"2025-09-06T22:41:18.010546Z","shell.execute_reply.started":"2025-09-06T20:48:04.735857Z","shell.execute_reply":"2025-09-06T22:41:18.009568Z"}},"outputs":[{"name":"stdout","text":"Loading and preprocessing data...\nTotal samples: 404287\nDuplicate pairs: 149263\nNon-duplicate pairs: 255024\nTrain samples: 258743\nValidation samples: 64686\nTest samples: 80858\n\n============================================================\nEXPERIMENT 1: BENCHMARK WITH DEFAULT WEIGHTS\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9910077fb8374f17b3b35f1b45f87256"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b86b7d5166247f5952dae7e201ae08a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d727eea81ae84524906700f32807f9db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df0911d688c44eebb42edd9701cb5faa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b667bb4403544ee4a8ec298b1c42d5af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95abc4830fc944c6b52b63fda852d0d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f334210c22cd4d52b48f18915ad0be67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de5e54ee74a843f798462158ff53fbaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa52fb9c36664435a14da25a7fef6556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e424c370e414fbdbee03afe3297f347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d869857b1d94515a937b576b2ea3d3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acbcd566628949389ea52c2766330e35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e173640cd8e4b33813f7820cbdf5693"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd489cccb4b04bb9b47fb28caa64dad6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d2e72f715534f5ea32e7095c25f7b05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6716afe8f6134a7d97fec24b17524973"}},"metadata":{}},{"name":"stdout","text":"Optimal threshold: 0.80 (Val F1: 0.5409)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1264 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd436ff6b2fc4605a86726236190c971"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1264 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83453add0d24408eaf07ca728c2fdc1a"}},"metadata":{}},{"name":"stdout","text":"Test F1-Score: 0.5409\nTest Precision: 0.3708\nTest Recall: 0.9996\nTest Accuracy: 0.3735\n\n============================================================\nEXPERIMENT 2: BI-ENCODER WITH COSINE SIMILARITY LOSS\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12129' max='12129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12129/12129 29:43, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Dev-score Pearson Cosine</th>\n      <th>Dev-score Spearman Cosine</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.185400</td>\n      <td>No log</td>\n      <td>0.520413</td>\n      <td>0.528578</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.215900</td>\n      <td>No log</td>\n      <td>0.315662</td>\n      <td>0.315943</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.315000</td>\n      <td>No log</td>\n      <td>0.010843</td>\n      <td>0.010843</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.298800</td>\n      <td>No log</td>\n      <td>-0.002611</td>\n      <td>-0.002611</td>\n    </tr>\n    <tr>\n      <td>4043</td>\n      <td>0.298800</td>\n      <td>No log</td>\n      <td>-0.005139</td>\n      <td>-0.005139</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.276600</td>\n      <td>No log</td>\n      <td>nan</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.266500</td>\n      <td>No log</td>\n      <td>0.004965</td>\n      <td>0.004965</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.265600</td>\n      <td>No log</td>\n      <td>nan</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.263700</td>\n      <td>No log</td>\n      <td>nan</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>8086</td>\n      <td>0.263700</td>\n      <td>No log</td>\n      <td>0.010572</td>\n      <td>0.010572</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.259700</td>\n      <td>No log</td>\n      <td>nan</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.250600</td>\n      <td>No log</td>\n      <td>nan</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.250500</td>\n      <td>No log</td>\n      <td>nan</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.251300</td>\n      <td>No log</td>\n      <td>nan</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>12129</td>\n      <td>0.251300</td>\n      <td>No log</td>\n      <td>nan</td>\n      <td>nan</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdc8fb1b11a541988cb51b49d890b795"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d81408bf39ac4b0a93efebba2c76b7fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a490b41fe7df498a959c415bba04529d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0bd7f4c77bb4108a70bf4c0997325b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d97e1d5b79404240adc0d29b2302a998"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50a1f9dcf0a44ec497138de57fce8989"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d985e9db9ca44683814ae21effed11c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09ce654a37354563a58c3faf7fe99095"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b18ad6eb74b34efa9bd6671fe29a59c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fe86cf31dc545dfaf717693475f629e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f1548b390a745e188db5901aa6e3810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23a747114dac425d92fa54a2bf96043e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4973af978932427184f7f901c46c9943"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de508f05f6b34076bbe9d35463f988ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73ff4f33a85648a4a241aaf884e5549f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b3620967dbc4a339db0d2d10ee06cef"}},"metadata":{}},{"name":"stdout","text":"Optimal threshold: 0.10 (Val F1: 0.5393)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1264 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ef486b3cdc549bbaaaf9b042978e0d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1264 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ac7f914376d4ad488b2bed86d396c5b"}},"metadata":{}},{"name":"stdout","text":"Test F1-Score: 0.5393\nTest Precision: 0.3692\nTest Recall: 1.0000\nTest Accuracy: 0.3692\n\n============================================================\nEXPERIMENT 3: BI-ENCODER WITH CONTRASTIVE LOSS\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12129' max='12129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12129/12129 29:49, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Dev-score Pearson Cosine</th>\n      <th>Dev-score Spearman Cosine</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.021400</td>\n      <td>No log</td>\n      <td>0.531084</td>\n      <td>0.572962</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.020700</td>\n      <td>No log</td>\n      <td>0.531327</td>\n      <td>0.592452</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.035200</td>\n      <td>No log</td>\n      <td>0.233092</td>\n      <td>0.321016</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.025700</td>\n      <td>No log</td>\n      <td>0.221416</td>\n      <td>0.260622</td>\n    </tr>\n    <tr>\n      <td>4043</td>\n      <td>0.025700</td>\n      <td>No log</td>\n      <td>0.207632</td>\n      <td>0.256810</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.030900</td>\n      <td>No log</td>\n      <td>0.197188</td>\n      <td>0.351729</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.032300</td>\n      <td>No log</td>\n      <td>-0.010270</td>\n      <td>0.055039</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.032100</td>\n      <td>No log</td>\n      <td>-0.012795</td>\n      <td>-0.018197</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.031900</td>\n      <td>No log</td>\n      <td>-0.006566</td>\n      <td>-0.038200</td>\n    </tr>\n    <tr>\n      <td>8086</td>\n      <td>0.031900</td>\n      <td>No log</td>\n      <td>-0.006995</td>\n      <td>-0.036657</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.031800</td>\n      <td>No log</td>\n      <td>-0.025747</td>\n      <td>-0.041094</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.031600</td>\n      <td>No log</td>\n      <td>0.006543</td>\n      <td>0.003767</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.031400</td>\n      <td>No log</td>\n      <td>nan</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.031200</td>\n      <td>No log</td>\n      <td>nan</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>12129</td>\n      <td>0.031200</td>\n      <td>No log</td>\n      <td>nan</td>\n      <td>nan</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e224fbc5fd1b43da950074156eab9c55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2255f2298934d7694ce38c2a71d82a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfcf8b9e7b794a60a6db59493cc5844c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d1909f5500a445f8efe87ac9eb5b0eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f333938adc82409db97518502d07017d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ca81d6874424bd1b8d5f17c9a1abdf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb100798d1ea4427b9b43d4941ac9dd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728c851c1e284c95bba6e25012cb7b5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cac4da32eadd4555b914702868df0e04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d3bf7acf6434c8397f1500dd7ab0b75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d091778a6a84e8da593878589b7fc7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46833c4b9cd84a838cf868b568b28821"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27737a9ff06f42368248cabf95ac77ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df98ee9779d44ba8b37f6e4f4bb93b8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db89b642126347a4b7e8cc29b8a157f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"907db91cc9bd4703ba97f4a8417c1fd2"}},"metadata":{}},{"name":"stdout","text":"Optimal threshold: 0.10 (Val F1: 0.5393)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1264 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2d124909b7e40f4a61019bc69e777f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1264 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baf495eb93aa4074ad1570c9e6bea24a"}},"metadata":{}},{"name":"stdout","text":"Test F1-Score: 0.5393\nTest Precision: 0.3692\nTest Recall: 1.0000\nTest Accuracy: 0.3692\n\n============================================================\nEXPERIMENT 4: BI-ENCODER WITH MULTIPLE NEGATIVE RANKING LOSS\n============================================================\nUsing 95528 positive pairs for MNR training\n","output_type":"stream"},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4479' max='4479' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4479/4479 11:47, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Dev-score Pearson Cosine</th>\n      <th>Dev-score Spearman Cosine</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.160300</td>\n      <td>No log</td>\n      <td>0.450004</td>\n      <td>0.513506</td>\n    </tr>\n    <tr>\n      <td>1493</td>\n      <td>0.160300</td>\n      <td>No log</td>\n      <td>0.458358</td>\n      <td>0.540480</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.098200</td>\n      <td>No log</td>\n      <td>0.471713</td>\n      <td>0.558528</td>\n    </tr>\n    <tr>\n      <td>2986</td>\n      <td>0.085900</td>\n      <td>No log</td>\n      <td>0.479256</td>\n      <td>0.557477</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.073900</td>\n      <td>No log</td>\n      <td>0.485345</td>\n      <td>0.574856</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.060100</td>\n      <td>No log</td>\n      <td>0.488860</td>\n      <td>0.573488</td>\n    </tr>\n    <tr>\n      <td>4479</td>\n      <td>0.060100</td>\n      <td>No log</td>\n      <td>0.489730</td>\n      <td>0.579213</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e583a5d547de49c9a37ac77c2181ed90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3d9364e25444e1e98ca8a93c57c031a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a81299dcb9234333b092acc620ef7fb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"715324f2f5e845b99b17419b32609faa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca2edff34096440e9f2f57f16de04b0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce0eb78af6204403b19914e32216c0a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"903709dbd2be444081d15d2952ef36cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1621c847e1114327a7b8cb7b10a9b296"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6acecd2b2eb846d799de1e5a3f8f6e41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fec7f149fa174d79b94491e934fa4396"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5d9a9063459455c9f125e88d2bc7ad9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1add28f7006246c1a1b4e09bb65f56e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9ba98794b8e44bc86803e37078a4937"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18a30a970a1d45d1ae463bfa92ea3fb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97c5da27c85432c9d216c66e0cffb0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1011 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de37559f7c604a81a0614b778932bdf2"}},"metadata":{}},{"name":"stdout","text":"Optimal threshold: 0.80 (Val F1: 0.7147)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1264 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ecaaba5c78d4a2daa9c5f871c515508"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1264 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66d2af05ca83426e9ec80ab2055fe5cc"}},"metadata":{}},{"name":"stdout","text":"Test F1-Score: 0.7174\nTest Precision: 0.5934\nTest Recall: 0.9071\nTest Accuracy: 0.7362\n\n============================================================\nEXPERIMENT 5: CROSS-ENCODER\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/xtremedistil-l6-h256-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2e4d2f7af8f42a9bd7800cb8c160296"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12129' max='12129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12129/12129 20:11, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Dev Accuracy</th>\n      <th>Dev Accuracy Threshold</th>\n      <th>Dev F1</th>\n      <th>Dev F1 Threshold</th>\n      <th>Dev Precision</th>\n      <th>Dev Recall</th>\n      <th>Dev Average Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.383000</td>\n      <td>No log</td>\n      <td>0.835436</td>\n      <td>0.527006</td>\n      <td>0.789192</td>\n      <td>0.459072</td>\n      <td>0.731966</td>\n      <td>0.856126</td>\n      <td>0.845576</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.380800</td>\n      <td>No log</td>\n      <td>0.847587</td>\n      <td>0.559932</td>\n      <td>0.802683</td>\n      <td>0.448868</td>\n      <td>0.747365</td>\n      <td>0.866845</td>\n      <td>0.858537</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.385800</td>\n      <td>No log</td>\n      <td>0.846706</td>\n      <td>0.599240</td>\n      <td>0.803125</td>\n      <td>0.508992</td>\n      <td>0.746146</td>\n      <td>0.869525</td>\n      <td>0.854656</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.374600</td>\n      <td>No log</td>\n      <td>0.844912</td>\n      <td>0.515844</td>\n      <td>0.799838</td>\n      <td>0.440410</td>\n      <td>0.741558</td>\n      <td>0.868060</td>\n      <td>0.858981</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.332700</td>\n      <td>No log</td>\n      <td>0.855842</td>\n      <td>0.579472</td>\n      <td>0.813977</td>\n      <td>0.491975</td>\n      <td>0.772748</td>\n      <td>0.859853</td>\n      <td>0.871908</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.338300</td>\n      <td>No log</td>\n      <td>0.857945</td>\n      <td>0.549539</td>\n      <td>0.814776</td>\n      <td>0.426916</td>\n      <td>0.765765</td>\n      <td>0.870488</td>\n      <td>0.875232</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.326300</td>\n      <td>No log</td>\n      <td>0.857481</td>\n      <td>0.588826</td>\n      <td>0.814575</td>\n      <td>0.504925</td>\n      <td>0.766092</td>\n      <td>0.869609</td>\n      <td>0.876066</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.314400</td>\n      <td>No log</td>\n      <td>0.864190</td>\n      <td>0.405110</td>\n      <td>0.822817</td>\n      <td>0.300565</td>\n      <td>0.777327</td>\n      <td>0.873964</td>\n      <td>0.878559</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.268700</td>\n      <td>No log</td>\n      <td>0.864422</td>\n      <td>0.652779</td>\n      <td>0.823051</td>\n      <td>0.506902</td>\n      <td>0.775110</td>\n      <td>0.877313</td>\n      <td>0.880129</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.273500</td>\n      <td>No log</td>\n      <td>0.866617</td>\n      <td>0.678871</td>\n      <td>0.827249</td>\n      <td>0.626775</td>\n      <td>0.785376</td>\n      <td>0.873838</td>\n      <td>0.888480</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.272100</td>\n      <td>No log</td>\n      <td>0.870946</td>\n      <td>0.450275</td>\n      <td>0.831878</td>\n      <td>0.384605</td>\n      <td>0.794039</td>\n      <td>0.873503</td>\n      <td>0.891584</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.260700</td>\n      <td>No log</td>\n      <td>0.870869</td>\n      <td>0.701720</td>\n      <td>0.831478</td>\n      <td>0.443363</td>\n      <td>0.782639</td>\n      <td>0.886819</td>\n      <td>0.890676</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b38bb33210704185983111b84396bba7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0934998067d14778bdd5a1f8ccc02b09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1379bff20f6a4fd09c551ae934e9ef46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5d49e81fd67466ca57b0062b2102870"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d11c276010e74ec881c42e587af5f382"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1ffb3d16d2e4c1e80d8d6382b20bab9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c24ba029c8948aca0e96aec5d5ee38c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1bbb82d3c804f93b839230a83d72d73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23d940f2e624471aaa8dc7afc6cc6377"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3419c244d4df4718a1c6188ce2a52ad0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e3093e164414e5dbabd0ef8b98c4a42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac472703f90e49eba5a28428729f231c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ee483994b44051af69d54f1a367058"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4be6f2629c7424883ab9a9e57d31a61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2022 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c953980026b4d29a56e963f27283ee9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0606b2a67c54d49a73aa72f88868e91"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3399118600.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;31m# Evaluate cross-encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m \u001b[0mopt_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_optimal_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cross-encoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Optimal threshold: {opt_threshold:.2f} (Val F1: {val_f1:.4f})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3399118600.py\u001b[0m in \u001b[0;36mfind_optimal_threshold\u001b[0;34m(model, val_dataset, model_type)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f1_score\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_f1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mbest_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f1_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3399118600.py\u001b[0m in \u001b[0;36mevaluate_model_performance\u001b[0;34m(model, test_dataset, model_type, threshold)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mtrue_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."],"ename":"IndexError","evalue":"invalid index to scalar variable.","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
